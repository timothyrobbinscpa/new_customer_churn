{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c861627-b25f-41c1-8788-f32e8adc4d47",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed526e8a-aedf-4fb2-9d4d-3f2b16a5d682",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07223180-063e-4524-9b3b-6b3a1fe9f8eb",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958915a-80b5-4c42-b391-545f45f31643",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Initial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5652c7-4248-430e-9903-c60024947fa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8673707-7b0b-4c07-ae19-40fe5c475cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d6d0fa-d713-4fe6-b952-41b1ea5e2372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/churn-bigml-80.csv')\n",
    "df_test = pd.read_csv('../data/churn-bigml-20.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaa1634d-4123-454c-864c-eed63ac1d360",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2666, 20)\n",
      "(667, 20)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae873c-df6f-486c-9354-408da79933fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d4cf4-08fc-4f9c-b54f-87ac69990605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fba923-b15c-421f-bcb9-6477e226f06a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891bbe3f-9a2c-4ca6-9c2e-13ff2f3933d8",
   "metadata": {},
   "source": [
    "## Initial Observations of Numeric Columns:\n",
    "\n",
    "- Account length: Average account length is about 101 days, with a standard deviation of approximately 39.6 days. The range is wide, from 1 to 243 days.\n",
    "- Area code: The area codes are limited to three values (408, 415, and 510), indicating categorical nature despite being numeric.\n",
    "- Number vmail messages: On average, customers have about 8 voicemail messages, but the standard deviation is quite high (13.6), indicating a wide distribution. The maximum number of voicemail messages recorded is 50.\n",
    "- Total Day, Evening, Night, and International Minutes and Calls: \n",
    "    - Total minutes and Total calls (Day, Evening, Night, and International): These columns show the total minutes and number of calls made during different times of the day. The average, standard deviation, minimum, \n",
    "    and maximum values vary for each period, which can provide insights into customer usage patterns.\n",
    "    - Total Charges (Day, Evening, Night, and International): Corresponding to the minutes and calls, these columns indicate the charges incurred. They show a similar distribution pattern to the minutes columns.\n",
    "- Customer Service Calls: Customers make an average of 1.56 service calls, with a standard deviation of 1.31. The maximum number of customer service calls made by a customer is 9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91182c8a-8e72-4706-83f4-8ac337dcf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe520691-1da0-4fa4-b2ab-686d6440d057",
   "metadata": {},
   "source": [
    "## Inital Observations of Categorical Columns\n",
    "\n",
    "- State: The dataset includes customers from various states, with the number of customers from each state varying. The highest representation is from the state of WV (West Virginia) with 88 customers, and the lowest from CA (California) with 24 customers.\n",
    "- International Plan: A majority of the customers do not have an international plan (2396 out of 2666), while 270 customers do.\n",
    "- Voice Mail Plan: 1933 customers do not have a voice mail plan, whereas 733 customers do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aacfb3a-159c-48ac-a3b2-c470d5f7a49f",
   "metadata": {},
   "source": [
    "### Update Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded5a995-bf84-49c8-9385-8eaa110d1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all column names to lowercase and add '_' between words\n",
    "df_train.columns = [col.lower().replace(' ', '_') for col in df_train.columns]\n",
    "df_test.columns = [col.lower().replace(' ', '_') for col in df_test.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87aaf7-d947-4f77-bc06-92b944c6a10d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0dec21-44c8-4de8-bddc-975d4e4f5aa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Missing and Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619c84b6-e393-4394-97d8-208d9c74a72e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "df_train.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6651f6ab-a727-420c-8125-152fe3e36f5a",
   "metadata": {},
   "source": [
    "No missing values in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbec170-f28e-42ab-9b88-3b63b3a3925b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce414aa2-caef-4627-9c9f-6e28c3c952d0",
   "metadata": {},
   "source": [
    "There are no duplicate values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26facb-dcfd-4b03-9da7-6cbe937636ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exploratory Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb93a7d-47b1-4261-8aac-af911e64a63a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a9400-7a3e-4cf0-a3e4-e4944b8d2490",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Distribution of Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0153b87-16c0-41d1-b5d6-381fa0078eb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show distribution of target variable\n",
    "\n",
    "sns.countplot(data=df_train, x='churn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156f884-3078-45f0-9518-435be9430b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analysis of the target variable 'churn'\n",
    "churn_distribution = df_train['churn'].value_counts(normalize=True)\n",
    "\n",
    "churn_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1848a-040e-4c0b-a77e-a462206af80b",
   "metadata": {},
   "source": [
    "Churned customers represent only 14.55% of the data.  Such an inbalance in the target variable can create problems in model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029724a-abb3-4764-9597-73e0fe522704",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b67cef-60ad-4b66-a9f8-e3766384fbdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_univariate_plots(dataframe):\n",
    "    ''' to print histograms and boxplots for numeric variables side-by-side '''\n",
    "    num_cols = dataframe.select_dtypes(include=['int', 'float']).columns\n",
    "\n",
    "    for col in num_cols:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        # Histogram\n",
    "        sns.histplot(dataframe, x=col, bins=dataframe[col].nunique(), ax=axes[0], kde=True, color='blue')\n",
    "        axes[0].set_title(f'Histogram of {col}')\n",
    "\n",
    "        # Boxplot\n",
    "        sns.boxplot(data=dataframe, x=col, ax=axes[1], color='lightgreen')\n",
    "        axes[1].set_title(f'Boxplot of {col}')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "show_univariate_plots(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee53531-db1c-4de9-976c-2242605b71fa",
   "metadata": {},
   "source": [
    "Here are some initial observations:\n",
    "\n",
    "- Account Length: The distribution seems roughly normal, centered around 100 days. There are no significant outliers.\n",
    "- Number of Voice Mail Messages: Many customers have zero voice mail messages, and the distribution is right-skewed. There are a few outliers with a very high number of messages.\n",
    "- Total Day Minutes: The distribution appears normal. There are a few outliers on both the lower and upper ends.\n",
    "- Total Day Calls: This variable also shows a roughly normal distribution with a few outliers.\n",
    "- Total Eve Minutes, Total Night Minutes: Both show normal distributions similar to 'Total Day Minutes', with some outliers.\n",
    "- Total Intl Minutes: The distribution is roughly normal, with some outliers on the higher end.\n",
    "- Customer Service Calls: Most customers made only a few calls to customer service, with the distribution being right-skewed. There are outliers with a high number of customer service calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a4d65e-97e1-47f7-b379-b9e24081bfa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e506039-fe68-447a-b409-60197fd7c352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjusting the dataset and column names for the univariate analysis of categorical variables\n",
    "cat_vars_updated = ['state', 'area_code', 'international_plan', 'voice_mail_plan']\n",
    "\n",
    "# Creating subplots for the categorical variables\n",
    "fig, axes = plt.subplots(nrows=len(cat_vars_updated), ncols=1, figsize=(10, 20), tight_layout=True)\n",
    "\n",
    "# Plotting bar plots for each categorical variable\n",
    "for i, var in enumerate(cat_vars_updated):\n",
    "    data_count = df_train[var].value_counts()\n",
    "    sns.barplot(x=data_count.index, y=data_count.values, ax=axes[i], palette=\"viridis\")\n",
    "    axes[i].set_title(f'Frequency Distribution of {var}')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].set_xlabel(var)\n",
    "\n",
    "    # Rotating x-axis labels for clarity if needed\n",
    "    if len(data_count.index) > 10:\n",
    "        for label in axes[i].get_xticklabels():\n",
    "            label.set_rotation(45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb92de7-9541-4bf6-9d07-2f70bae0583b",
   "metadata": {},
   "source": [
    "Categorical Data Frequencies:\n",
    "\n",
    "- State: The distribution across states is shown, with the number of customers in each state varying slightly but generally being quite balanced.\n",
    "- Area Code: This plot shows the frequency of each area code in the dataset.\n",
    "- International Plan: It's apparent that the majority of customers do not have an international plan.\n",
    "- Voice Mail Plan: Similarly, most customers do not have a voice mail plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8208bb6b-ccb4-421f-bd2f-7952c73469e8",
   "metadata": {},
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ba6ac-b8ba-41de-b817-daaa6437941b",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc14a0fd-7e36-401d-8530-67b5fccdae45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check correlations with target and other variables\n",
    "\n",
    "corr_matrix = df_train.select_dtypes(include=['number'])\n",
    "#corr_matrix = df_train\n",
    "\n",
    "corr_matrix = corr_matrix.reset_index(drop=True)\n",
    "\n",
    "heat = corr_matrix.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846c7e9-4387-4962-b726-402a5110e937",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "heat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661dad3c-f000-4989-b504-3d8b8f51d42f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(heat, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efbfc6-4ea6-44cc-acdb-beefa8f795c7",
   "metadata": {},
   "source": [
    "#### Redundant Features\n",
    "Although some of the variables, such as total_day_charge and total_day_minutes, are highly correlated, models like Random Forest and Gradient Boosting, are not negatively impacted by multicollinearity. These models can handle correlated features without performance degradation.  Also, each variable might contain unique information. Removing one of two correlated variables can sometimes lead to a loss of valuable information.  Therefore, we will retain these variables.\n",
    "\n",
    "There are no significant correlations between any of the feature variables; some small correlations with target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae513e-8062-4788-8eae-3db660546a69",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Key Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2cd99a-03dd-45d3-8986-9547db837d25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis: Numerical vs Categorical (Churn)\n",
    "num_vars_for_bivariate = ['total_day_minutes', 'total_eve_minutes', 'total_night_minutes', 'customer_service_calls']\n",
    "\n",
    "# Creating subplots for the box plots\n",
    "fig, axes = plt.subplots(nrows=len(num_vars_for_bivariate), ncols=1, figsize=(10, 15), tight_layout=True)\n",
    "\n",
    "# Creating box plots for each numerical variable against churn\n",
    "for i, var in enumerate(num_vars_for_bivariate):\n",
    "    sns.boxplot(x='churn', y=var, data=df_train, ax=axes[i], palette=\"Set2\")\n",
    "    axes[i].set_title(f'{var} vs churn')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e81590-b7aa-4981-96f1-fe86017f06a6",
   "metadata": {},
   "source": [
    "- Total Day Minutes: There is a noticeable difference in the distribution of total day minutes between customers who churned and those who did not. Customers who churned tend to have higher total day minutes.\n",
    "- Total Eve Minutes: The distributions between churned and non-churned customers for evening minutes are less distinct compared to day minutes, but there still appears to be a slight difference.\n",
    "- Total Night Minutes: The distributions of total night minutes for churned and non-churned customers are quite similar, suggesting this might be less indicative of churn.\n",
    "- Customer Service Calls: There is a clear difference in the number of customer service calls between the two groups. Customers who churned tend to have made more customer service calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19880abe-2d61-4612-8589-278eb08d4d9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis: Categorical vs Numerical (International Plan vs Total Day Minutes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='international_plan', y='total_day_minutes', data=df_train, palette=\"Set1\")\n",
    "plt.title(\"Total Day Minutes vs International Plan\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd526207-2a02-4659-ba59-338775b4de42",
   "metadata": {
    "tags": []
   },
   "source": [
    "Total Day Minutes vs International Plan:\n",
    "- The box plot shows the distribution of total_day_minutes for customers with and without an international plan.\n",
    "- It appears that customers with an international plan tend to have slightly higher day minute usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebaf78a-d833-4dab-bd15-23c3d0a663ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Key Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa50196a-c1be-47f6-942e-503cd9275c07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis: Categorical vs Categorical (International Plan vs Churn)\n",
    "# Calculating the proportions for a better comparison\n",
    "churn_by_international_plan = pd.crosstab(df_train['international_plan'], df_train['churn'], normalize='index')\n",
    "churn_by_international_plan.plot(kind='bar', stacked=True, color=[\"green\", \"red\"])\n",
    "plt.title(\"Churn by International Plan\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068973db-4e56-42e2-b94f-16bf611a39ed",
   "metadata": {},
   "source": [
    "Churn by International Plan:\n",
    "- The stacked bar chart illustrates the proportion of churned and non-churned customers within each category of the international_plan.\n",
    "- There's a noticeable difference in churn rates between customers with and without an international plan. A higher proportion of customers with an international plan tend to churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f2ed7-1208-4e64-a3e4-a90f2cc69b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis: Categorical vs Categorical (Voice Mail Plan vs Churn)\n",
    "churn_by_voice_mail_plan = pd.crosstab(df_train['voice_mail_plan'], df_train['churn'], normalize='index')\n",
    "churn_by_voice_mail_plan.plot(kind='bar', stacked=True, color=[\"blue\", \"orange\"])\n",
    "plt.title(\"Churn by Voice Mail Plan\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d1137-8aa7-443f-95fa-6300b346a77a",
   "metadata": {},
   "source": [
    "Churn by Voice Mail Plan:\n",
    "- The stacked bar chart shows the proportion of churned and non-churned customers relative to whether they have a voice mail plan.\n",
    "- Interestingly, the proportion of churn appears to be lower among customers with a voice mail plan compared to those without."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a86c5-3da2-446d-afec-eab16f3371f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis: Numerical vs Categorical (Customer Service Calls vs Churn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='churn', y='customer_service_calls', data=df_train, palette=\"Set3\")\n",
    "plt.title(\"Customer Service Calls vs Churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db062b2-9e4e-4cf4-b0c0-c62aa86526ad",
   "metadata": {},
   "source": [
    "Customer Service Calls vs Churn:\n",
    "- The box plot compares the number of customer service calls made by churned and non-churned customers.\n",
    "- There's a clear distinction: customers who churned tend to have made more customer service calls. This indicates a strong relationship between the number of customer service calls and churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42719196-4f66-49e8-bc9f-36a4ee2c6b39",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89909954-ef92-4ecf-9208-511ff2026342",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d094c-704c-4548-9921-eff42a6f0abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "for column in ['state', 'international_plan', 'voice_mail_plan']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df_train[column] = label_encoders[column].fit_transform(df_train[column])\n",
    "    df_test[column] = label_encoders[column].transform(df_test[column])\n",
    "\n",
    "# Split the data into features and target\n",
    "X_train = df_train.drop('churn', axis=1)\n",
    "y_train = df_train['churn']\n",
    "X_test = df_test.drop('churn', axis=1)\n",
    "y_test = df_test['churn']\n",
    "\n",
    "# Apply Robust Scaling\n",
    "scaler = RobustScaler()\n",
    "numerical_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "X_train_scaled[numerical_columns] = scaler.fit_transform(X_train[numerical_columns])\n",
    "X_test_scaled[numerical_columns] = scaler.transform(X_test[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc9333-3cdf-4157-bd67-bf89b3b4c474",
   "metadata": {},
   "source": [
    "Due to the large number of ouliers discovered during EDA, I chose to apply robust scaling when preprocessing the data.  Robust scaling is a technique used to scale features to a similar range by using statistics that are robust to outliers. It's particularly useful in datasets where the presence of outliers is expected or when outliers cannot be removed because they contain valuable information. \n",
    "\n",
    "Robust scaling reduces the influence of outliers in the data since it uses the median and the interquartile range (IQR) for scaling. These are less sensitive to outliers than the mean and variance used in standard scaling.  In many real-world scenarios, outliers can contain important information. Robust scaling allows you to retain these data points without letting them dominate the feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0fb33f-9986-45d0-9a45-fe0cb26d30b3",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20badafd-df36-415f-8f3e-3d9cca6ae9fc",
   "metadata": {},
   "source": [
    "I chose to use a Random Forest model as it has many benefits when it comes down to the complexities of predicting customer churn:\n",
    "\n",
    "- Handling Imbalanced Data: As is the case with our dataset, it is imbalanced since the number of customers who don't churn outweighs those who churn.  Random Forest can handle imbalances well because it involves averaging multiple decision trees to make a final prediction, which can dilute the effect of imbalanced classes.\n",
    "- Feature Importance: Random Forest can provide insights into which features are most important for predicting churn. This is valuable for understanding the underlying factors driving customer churn and can inform business strategies to improve customer retention.\n",
    "- Non-Linear Relationships: Random Forest can naturally model non-linear relationships between features without the need for transformation, which is common in customer data.\n",
    "- Interaction Effects: It can capture interaction effects between features without explicit feature engineering. Customer churn is often influenced by complex interactions between different customer attributes and behaviors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dc5a53-1a20-46f3-ad22-45bd0a5be06d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd51743e-ae39-4cf1-bd02-9eac13f35190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit initial model\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print({\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761e900-a6e7-4ece-b718-6d6f3ebf9501",
   "metadata": {},
   "source": [
    "The initial Random Forest model performed well in terms of accuracy and precision.  However, the recall score is pretty low.  Since recall measures the model's ability to identify all actual positives, a high recall means the model is good at identifying customers who will churn.  In this case, I believe that prioritizing recall is more crucial, since the cost of losing a customer (who could have been retained) can be much higher than the cost of replacing the customer. This approach seeks to minimize missed opportunities for retaining customers at risk of churning, even if it means some false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8e241-8072-4740-a521-b059140792f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the weight ratios\n",
    "\n",
    "# Generate a range of weight ratios for the minority class (churned = True)\n",
    "weight_ratios = np.linspace(0.1, 12.5, 50)  # Adjust the range and number of ratios as needed\n",
    "\n",
    "# Create a list of dictionaries for class_weight\n",
    "class_weight_options = [{0: 1, 1: ratio} for ratio in weight_ratios]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b01d27-6884-473c-87eb-483538e76974",
   "metadata": {},
   "source": [
    "Using weight ratios in a Random Forest model for customer churn is particularly beneficial when dealing with imbalanced datasets, where the number of churned customers is significantly lower than the number of retained customers. \n",
    "- Correcting Imbalance: Weight ratios can help correct the class imbalance by assigning a higher weight to the minority class (churned customers). This tells the model that errors in predicting the minority class are more costly than errors in predicting the majority class.\n",
    "- Improving Recall for the Minority Class: By giving more weight to the minority class, the model will focus more on correctly predicting those cases, potentially increasing the recall. This is important in churn prediction because failing to identify a customer at risk of churn could be more costly than incorrectly identifying a satisfied customer as at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a3640-e91c-4c2b-ad31-660e3641a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameter grid for Randomized Search\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': np.linspace(100, 1000, 10, dtype=int),  \n",
    "    'max_depth': np.linspace(10, 100, 10, dtype=int),      \n",
    "    'min_samples_split': np.linspace(2, 10, 5, dtype=int), \n",
    "    'min_samples_leaf': np.linspace(1, 5, 5, dtype=int),      \n",
    "    'bootstrap': [True, False],       \n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'class_weight': class_weight_options\n",
    "}\n",
    "\n",
    "# Creating a RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_dist, \n",
    "    n_iter=50,  \n",
    "    scoring='recall',  # Focusing on recall\n",
    "    cv=3,       \n",
    "    verbose=2, \n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Using all available cores\n",
    ")\n",
    "\n",
    "# Performing random search\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "# Best parameters and best recall score\n",
    "best_params = random_search.best_params_\n",
    "best_recall = random_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Recall Score:\", best_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4511a73-c114-4c99-b33e-bf5d3a50bac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the Random Forest model with the best parameters found\n",
    "best_rf_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred)\n",
    "    recall = recall_score(y, y_pred)\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    return {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = evaluate_model(best_rf_model, X_test_scaled, y_test)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f3b81-c697-4370-948b-f3520c43e3c9",
   "metadata": {},
   "source": [
    "## Gradient Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33d1f8-4251-4f64-adc9-9e8bbcde894b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initial model\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming X_train_scaled, y_train, X_test_scaled, y_test are already defined from Random Forest preprocessing\n",
    "\n",
    "# Initial Gradient Boosting Model\n",
    "gb_initial = GradientBoostingClassifier(random_state=42)\n",
    "gb_initial.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate initial model\n",
    "initial_predictions = gb_initial.predict(X_test_scaled)\n",
    "print(\"Initial Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, initial_predictions)}\")\n",
    "print(f\"Precision: {precision_score(y_test, initial_predictions)}\")\n",
    "print(f\"Recall: {recall_score(y_test, initial_predictions)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, initial_predictions)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec8614-6eaf-44d5-8fb1-e461f099bdc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameter grid for Randomized Search\n",
    "param_dist_gb = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': np.arange(3, 11),\n",
    "    'min_samples_split': np.arange(2, 11),\n",
    "    'min_samples_leaf': np.arange(1, 20),\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],  # Subsampling the training set may help with imbalance\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "\n",
    "# Randomized Search\n",
    "random_search_gb = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_distributions=param_dist_gb, \n",
    "    n_iter=50, \n",
    "    scoring='recall', \n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search_gb.fit(X_train_scaled, y_train)\n",
    "best_params_gb = random_search_gb.best_params_\n",
    "best_recall_gb = random_search_gb.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params_gb)\n",
    "print(\"Best Recall Score:\", best_recall_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d6e76-73d3-4c16-bf39-99499190e4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and predict with the best parameters\n",
    "gb_best = GradientBoostingClassifier(**best_params_gb, random_state=42)\n",
    "gb_best.fit(X_train_scaled, y_train)\n",
    "best_predictions = gb_best.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate best model\n",
    "print(\"Best Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, best_predictions)}\")\n",
    "print(f\"Precision: {precision_score(y_test, best_predictions)}\")\n",
    "print(f\"Recall: {recall_score(y_test, best_predictions)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, best_predictions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e82e9-027b-4b28-96af-cbf56f6fc0aa",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b6bb9-749b-44cc-9c20-30640ace4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Assuming that X_test_scaled, y_test are already defined and best_rf_model, gb_best are trained\n",
    "\n",
    "# Evaluate Random Forest Model\n",
    "rf_metrics = evaluate_model(best_rf_model, X_test_scaled, y_test)\n",
    "\n",
    "# Evaluate Gradient Boosting Model\n",
    "gb_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_test, best_predictions),\n",
    "    \"Precision\": precision_score(y_test, best_predictions),\n",
    "    \"Recall\": recall_score(y_test, best_predictions),\n",
    "    \"F1 Score\": f1_score(y_test, best_predictions)\n",
    "}\n",
    "\n",
    "# Data for plotting\n",
    "labels = list(rf_metrics.keys())\n",
    "rf_values = list(rf_metrics.values())\n",
    "gb_values = list(gb_metrics.values())\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "# Bar Chart\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, rf_values, width, label='Random Forest')\n",
    "rects2 = ax.bar(x + width/2, gb_values, width, label='Gradient Boosting')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Model Performance Comparison')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "\n",
    "# Add value labels on bars\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00e5e66-9e08-4e95-a823-a40523c87963",
   "metadata": {},
   "source": [
    "These metrics in the above bar chart are crucial for evaluating the performance of classification models, especially in applications like customer churn prediction where both identifying churners (recall) and being correct in the prediction (precision) are important. Here's the analysis of the results:\n",
    "- Accuracy: Both models exhibit high accuracy, with Gradient Boosting slightly outperforming Random Forest (0.96 vs. 0.95). High accuracy means that both models correctly predict churn and non-churn customers a majority of the time.\n",
    "- Precision: Gradient Boosting has a precision of 0.93, which is higher than Random Forest's 0.81. This indicates that Gradient Boosting is better at correctly identifying customers who will churn, with fewer false positives (customers who were predicted to churn but did not).\n",
    "- Recall: Random Forest has a recall of 0.82, which is just slightly higher than Gradient Boosting's 0.80. This suggests that Random Forest is marginally better at identifying all actual churn cases, which is critical in situations where it's important to capture as many churners as possible for interventions.\n",
    "- F1 Score: The F1 Score, which balances precision and recall, is higher for Gradient Boosting (0.86) compared to Random Forest (0.82). A higher F1 Score indicates that Gradient Boosting has a better overall balance between precision and recall.\n",
    "\n",
    "In terms of customer churn prediction:\n",
    "- If the business goal is to identify as many potential churners as possible without being overly concerned about false positives, Random Forest might be the better choice due to its slightly higher recall.\n",
    "- If the business aims to target churn interventions more precisely and wishes to minimize the costs associated with false positives, then Gradient Boosting is preferable due to its higher precision.\n",
    "- Given the F1 Score, if the business seeks a balance between identifying churners and minimizing false positives, Gradient Boosting would be the best model overall.\n",
    "- The slightly higher accuracy of Gradient Boosting also suggests it might be the more robust model overall for this particular dataset.\n",
    "Ultimately, the choice between the two models should be guided by the specific costs associated with false positives and false negatives and the overall business strategy for customer retention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb805aa8-a131-423e-992c-d2c22e22ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall AUC Curve for both models\n",
    "y_score_rf = best_rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, y_score_rf)\n",
    "auc_rf = auc(recall_rf, precision_rf)\n",
    "\n",
    "y_score_gb = gb_best.predict_proba(X_test_scaled)[:, 1]\n",
    "precision_gb, recall_gb, _ = precision_recall_curve(y_test, y_score_gb)\n",
    "auc_gb = auc(recall_gb, precision_gb)\n",
    "\n",
    "# Plotting Precision-Recall curves\n",
    "plt.figure()\n",
    "plt.plot(recall_rf, precision_rf, label=f'Random Forest (PR AUC = {auc_rf:.2f})')\n",
    "plt.plot(recall_gb, precision_gb, label=f'Gradient Boosting (PR AUC = {auc_gb:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23aadac-5296-4fee-bf26-21a30418c6bd",
   "metadata": {},
   "source": [
    "The above graph is a Precision-Recall Curve that compares the performance of the two models - Random Forest and Gradient Boosting - in predicting customer churn. Here's how to interpret the graph:\n",
    "\n",
    "- Precision and Recall Trade-off: The Precision-Recall Curve illustrates the trade-off between precision (the proportion of true positives among all predicted positives) and recall (the proportion of true positives among all actual positives).  Ideally, a model would have both high precision and high recall, but typically as recall increases, precision decreases, and vice versa.\n",
    "- PR AUC (Area Under the Precision-Recall Curve): The area under the curve (AUC) for each model gives a single metric that sums up the model performance across all classification thresholds. A higher AUC represents a better overall performance of a model. In the attached graph, Gradient Boosting has a slightly higher PR AUC (0.88) compared to Random Forest (0.87), indicating that it generally achieves a better balance between precision and recall across different thresholds.\n",
    "\n",
    "Model Comparison: \n",
    "- Random Forest: With a PR AUC of 0.87, the Random Forest model is a strong performer, but the curve shows that for very high recall values, the precision drops significantly. This might indicate that while it can identify most churners, it does so at the expense of incorrectly predicting churn for a number of non-churners.\n",
    "- Gradient Boosting: The Gradient Boosting model has a PR AUC of 0.88, suggesting it is slightly more effective at distinguishing between churners and non-churners over a range of thresholds. It maintains a better precision for a given recall value, which means it can identify a high proportion of actual churners while keeping the number of false positives lower.\n",
    "\n",
    "Business Implications:\n",
    "\n",
    "A business that aims to minimize the cost of false positives (e.g., the cost of unnecessary retention offers) might prefer the Gradient Boosting model due to its higher precision at most recall levels. If the cost of missing out on actual churners (false negatives) is higher (e.g., the lost revenue from churned customers), a model with higher recall might be preferred. In this case, Random Forest and Gradient Boosting are quite close, but Gradient Boosting edges out slightly.\n",
    "\n",
    "In summary, for customer churn prediction, the choice between Random Forest and Gradient Boosting should be informed by the relative costs of false positives versus false negatives and the desired balance between precision and recall. Gradient Boosting has a slight edge in overall performance according to the PR AUC, but the best choice may vary depending on the specific context and business objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e59772-d519-4245-b937-31caf65ab1af",
   "metadata": {},
   "source": [
    "# Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825027c2-543d-4b4a-a7d1-bd069757d986",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "rf_importances = best_rf_model.feature_importances_\n",
    "gb_importances = gb_best.feature_importances_\n",
    "\n",
    "# Create a DataFrame for feature importances\n",
    "feature_names = X_train.columns\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Random Forest': rf_importances,\n",
    "    'Gradient Boosting': gb_importances\n",
    "})\n",
    "\n",
    "# Sort by the Random Forest importances for consistent ordering\n",
    "importances_df.sort_values('Random Forest', ascending=True, inplace=True)\n",
    "\n",
    "# Plotting the horizontal bar chart with increased vertical size\n",
    "fig, ax = plt.subplots(figsize=(10, len(feature_names) * 0.4))  # Adjust the multiplier for vertical size\n",
    "\n",
    "# Get the y-location for each algorithm's set of features\n",
    "y_locs_rf = np.arange(len(feature_names)) + 0.2  # RF bars above\n",
    "y_locs_gb = np.arange(len(feature_names)) - 0.2  # GB bars below\n",
    "\n",
    "# Plot the Random Forest importances\n",
    "ax.barh(y_locs_rf, importances_df['Random Forest'], height=0.4, label='Random Forest', color='royalblue')\n",
    "\n",
    "# Plot the Gradient Boosting importances\n",
    "ax.barh(y_locs_gb, importances_df['Gradient Boosting'], height=0.4, label='Gradient Boosting', color='seagreen')\n",
    "\n",
    "# Add the feature names on the y-axis\n",
    "ax.set_yticks(np.arange(len(feature_names)))\n",
    "ax.set_yticklabels(importances_df['Feature'])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Feature Importance')\n",
    "ax.set_title('Comparison of Feature Importances Between Random Forest and Gradient Boosting')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcf0018-a5cd-4135-90c6-02307c3bcd71",
   "metadata": {},
   "source": [
    "Recommended Strategies:\n",
    "\n",
    "Customer Service Calls\n",
    "Strategy: Improve Customer Support and Follow-Up. High numbers of service calls may indicate customer dissatisfaction or issues with the service. Focus on improving the quality of customer support. Implement follow-up procedures to ensure that problems are fully resolved. Analyzing the reasons behind these calls can provide insights into common issues and areas for improvement.\n",
    "Proactive Engagement: Use data analytics to identify customers who make frequent service calls and proactively reach out to them with solutions, offers, or assurances.\n",
    "\n",
    "Total Day Minutes and Total Day Charge\n",
    "Customized Plans: Since total day minutes and charges are significant, consider offering more personalized or flexible pricing plans that cater to customers' usage patterns. This could include tiered pricing or pay-as-you-go options.\n",
    "Usage-Based Discounts: Introduce discounts or special offers that reward high usage, encouraging customer loyalty and making high usage financially more attractive.\n",
    "Transparent Billing: Ensure that billing is transparent and easy to understand. Confusion over charges can be a significant factor in churn. Clear, detailed billing can improve trust and satisfaction.\n",
    "International Plan\n",
    "\n",
    "CONSIDER COMPARING RATES PER HOUR FOR EACH PLAN TYPE TO EACH OTHER - SAME WITH WHETHER THEY HAVE AN INTERNATIONAL PLAN\n",
    "\n",
    "Competitive International Offers: If having an international plan is a strong churn predictor, review your international plan offerings. They may be uncompetitive or unsatisfactory. Offering more attractive international rates or bundled international minutes could retain customers looking to use international services.\n",
    "Targeted Promotions for Frequent International Users: Use customer usage data to identify customers who would benefit most from an international plan and target them with special offers or promotions.\n",
    "\n",
    "In all these strategies, data analysis plays a critical role. Understanding the underlying reasons why these features are influential in predicting churn will enable more effective, targeted strategies. Additionally, customer segmentation, personalized communication, and leveraging customer feedback can further refine these strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
